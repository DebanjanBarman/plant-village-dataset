{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78883f51-1e49-4373-9fb1-359c7d822bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "json_file_path = \"./Plant_Disease_Dataset_Unified\"\n",
    "print(os.path.exists(json_file_path))  # This should return True if the file exists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba48b27-26de-4924-bd6c-959d01f56c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ea0ce4-d2cb-402e-b5ae-dd56e57993a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c1a859-fd52-4c26-89a7-a0da002781c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install segmentation-models-pytorch\n",
    "!pip install segmentation-models-pytorch torch torchvision albumentations\n",
    "!pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae8ad05-f58f-4463-9c34-c8637bd265e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88853b0-b878-48ea-8cd0-2c4935733102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e631e5-0737-47e4-b697-fb05b4d82779",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd4f849-ee8f-4705-acf0-71098d4968c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e28263e-1c12-4185-969b-9f66016ca7e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52335ec-8fcc-4229-a8e2-ba439aa341a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f773e9a-b15c-4127-b961-9299da30c3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(os.listdir(\"./segment-anything\"))\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "# Load SAM model\n",
    "\n",
    "sam_checkpoint = \"segment-anything/sam_vit_h_4b8939.pth\"  # Path to SAM weights model_type = \"vit_h\"  # Use \"vit_b\" for smaller models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)\n",
    "sam.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6bffde-f942-4491-b719-b178a334cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "\n",
    "class PlantDiseaseDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.mask_paths = []\n",
    "\n",
    "        # Load all image and mask paths\n",
    "        for class_name in os.listdir(root_dir):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                img_path = os.path.join(class_dir, img_name)\n",
    "                mask_path = img_path.replace(\"images\", \"masks\")  # Assuming masks are stored separately\n",
    "                self.image_paths.append(img_path)\n",
    "                self.mask_paths.append(mask_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        mask_path = self.mask_paths[idx]\n",
    "    \n",
    "        # Load image and mask\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert to RGB\n",
    "        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)  # Load as grayscale\n",
    "    \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']  # Already a tensor due to ToTensorV2()\n",
    "            mask = augmented['mask']    # Already a tensor due to ToTensorV2()\n",
    "    \n",
    "        # Add channel dimension to mask\n",
    "        mask = mask.unsqueeze(0)  # Shape: (1, H, W)\n",
    "    \n",
    "        return image, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4f0387-293b-4031-81b0-cb820269379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(256, 256),  # Resize to 256x256\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.RandomBrightnessContrast(p=0.2),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),  # ImageNet normalization\n",
    "    ToTensorV2(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049ecbc4-e89d-48e4-ba6f-3ae096397c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Define dataset paths\n",
    "train_dataset = PlantDiseaseDataset(root_dir=\"Plant_Disease_Dataset_Unified/train\", transform=transform)\n",
    "valid_dataset = PlantDiseaseDataset(root_dir=\"Plant_Disease_Dataset_Unified/valid\", transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0aff34-8a28-4b66-af6a-597a2bfa06a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PlantDiseaseDataset(root_dir=\"Plant_Disease_Dataset_Unified/train\", transform=transform)\n",
    "image, mask = dataset[0]  # Get the first sample\n",
    "print(\"Image shape:\", image.shape)  # Should be (3, H, W)\n",
    "print(\"Mask shape:\", mask.shape)   # Should be (1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51571681-3d89-4ff9-a35e-7504c3a35584",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, masks in train_loader:\n",
    "    print(\"Images shape:\", images.shape)\n",
    "    print(\"Masks shape:\", masks.shape)\n",
    "    break  # Stop after one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49428316-9f42-4113-8bf3-68e3525305c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "def generate_mask(image_path, output_path):\n",
    "    # Load an image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image at path: {image_path}\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize SAM predictor\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # Define a point prompt\n",
    "    input_point = np.array([[100, 100]])  # Point on diseased region\n",
    "    input_label = np.array([1])  # 1 for foreground\n",
    "\n",
    "    # Generate mask\n",
    "    masks, _, _ = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        multimask_output=True,  # Enable multi-mask output\n",
    "    )\n",
    "\n",
    "    # Save the mask\n",
    "    mask_image = Image.fromarray((masks[0] * 255).astype(np.uint8))\n",
    "    mask_image.save(output_path)\n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    generate_mask(\"path/to/image.jpg\", \"path/to/output/mask.png\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8beea-551b-40b0-8979-b750ae4d2b87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e768eee-7e9c-4696-ac4d-3e2b82361d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################ USING SAM ###################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d034b08-3cd2-4509-b9b5-6c98872bc546",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca348237-9d0f-40fe-b564-9060a5ad929b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63976561-7df9-4433-9ce2-9d4b7551bc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134407a7-5601-4d25-9904-824b946ade2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d3a6ca-52a9-4a54-a26a-a79feff40233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Is CUDA available?\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b37b115-4e45-4c21-9750-9167edb6d7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "print(os.listdir(\"./segment-anything\"))\n",
    "from segment_anything import sam_model_registry\n",
    "\n",
    "# Load SAM model\n",
    "\n",
    "sam_checkpoint = \"./segment-anything/sam_vit_h_4b8939.pth\"  # Path to SAM weights model_type = \"vit_h\"  # Use \"vit_b\" for smaller models\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923e538-e75e-4548-bf49-62c993bd3c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "# sam = sam_model_registry[\"vit_h\"](checkpoint=\"./segment-anything/sam_vit_h_4b8939.pth\")\n",
    "predictor = SamPredictor(sam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c7c2ac-e203-468d-bddf-e59added9389",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diseased_boxes(image):\n",
    "    hsv = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "\n",
    "    # Heuristic for diseased regions (brown/dark)\n",
    "    lower_brown = np.array([5, 50, 50])\n",
    "    upper_brown = np.array([30, 255, 200])\n",
    "\n",
    "    lower_black = np.array([0, 0, 0])\n",
    "    upper_black = np.array([180, 255, 50])\n",
    "\n",
    "    mask_brown = cv2.inRange(hsv, lower_brown, upper_brown)\n",
    "    mask_black = cv2.inRange(hsv, lower_black, upper_black)\n",
    "    combined_mask = cv2.bitwise_or(mask_brown, mask_black)\n",
    "\n",
    "    # Find contours of diseased regions\n",
    "    contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    boxes = []\n",
    "\n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        if w * h > 100:  # Filter small noise\n",
    "            boxes.append([x, y, x+w, y+h])  # Format: [x1, y1, x2, y2]\n",
    "\n",
    "    if not boxes:\n",
    "        raise ValueError(\"No diseased regions detected for box prompting.\")\n",
    "\n",
    "    return np.array(boxes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c126387-50cd-4184-8e67-9144f8ab01d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a71a5f-12aa-4b08-850a-4556a72686b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cc8da2-fe5f-480b-b03b-faad19cad4e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb03b384-341d-4c43-ac28-ceda3f113123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_mask(image_path, output_path):\n",
    "    # Load an image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        raise FileNotFoundError(f\"Could not load image at path: {image_path}\")\n",
    "\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Initialize SAM predictor\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    # Define a point prompt\n",
    "# Inside generate_mask()\n",
    "\n",
    "    boxes = get_diseased_boxes(image)\n",
    "    masks, _, _ = predictor.predict(\n",
    "        box=boxes[0],  # Use the first detected box\n",
    "        multimask_output=True\n",
    "    )\n",
    "    \n",
    "    best_mask = max(masks, key=lambda m: m.sum())\n",
    "\n",
    "    # Save the mask\n",
    "    mask_image = Image.fromarray((best_mask * 255).astype(np.uint8))\n",
    "    mask_image.save(output_path)\n",
    "\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        cv2.rectangle(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.title(\"Auto-detected Box Prompts for SAM\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "    # Show the image and the mask\n",
    "    plt.imshow(image)\n",
    "    plt.imshow(best_mask, alpha=0.5, cmap='jet')\n",
    "    plt.title(\"Mask Overlay\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Example usage\n",
    "try:\n",
    "    generate_mask(\"./Plant_Disease_Dataset_Unified/train/images/Grape___Black_rot/00cab05d-e87b-4cf6-87d8-284f3ec99626___FAM_B.Rot 3244.jpg\", \"./Plant_Disease_Dataset_Unified/train/images/Grape___Black_rot/masks/00cab05d-e87b-4cf6-87d8-284f3ec99626___FAM_B.Rot 3244.png\")\n",
    "except FileNotFoundError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5011cc43-5281-4610-b409-c830e4c7af4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18863c5-f63c-4733-add7-f2456204acb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc037c1-658f-4a91-b83b-5a67338cbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################  USE UNET #############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90aee11-c58e-4e19-8e21-98fc0565dc1e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b326ee7-9f75-4830-9e9a-8b34c6bc9d9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "\n",
    "# ----------------- Dataset -----------------\n",
    "class LeafSegmentationDataset(Dataset):\n",
    "    def __init__(self, image_paths, mask_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.mask_paths = mask_paths\n",
    "        self.transform = transform  # Fix: make sure this is initialized\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = np.array(Image.open(self.image_paths[idx]).convert(\"RGB\"))\n",
    "        mask = np.array(Image.open(self.mask_paths[idx]).convert(\"L\")) / 255.0\n",
    "\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=img, mask=mask)\n",
    "            img = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "        else:\n",
    "            img = torch.tensor(img / 255.0, dtype=torch.float).permute(2, 0, 1)\n",
    "            mask = torch.tensor(mask, dtype=torch.float).unsqueeze(0)\n",
    "\n",
    "        return img, mask\n",
    "\n",
    "\n",
    "# ----------------- Paths Matching -----------------\n",
    "def get_valid_image_mask_pairs():\n",
    "    # Only get images that have masks\n",
    "    mask_paths = glob(\"./dataset/masks/*/*.png\")\n",
    "    image_base_path = \"./Plant_Disease_Dataset_Unified/train/images\"\n",
    "\n",
    "    image_mask_pairs = []\n",
    "    for mask_path in mask_paths:\n",
    "        mask_name = os.path.splitext(os.path.basename(mask_path))[0]\n",
    "        \n",
    "        # Search for matching image in all subfolders\n",
    "        matching_image = glob(f\"{image_base_path}/*/{mask_name}.jpg\")\n",
    "        if matching_image:\n",
    "            image_mask_pairs.append((matching_image[0], mask_path))\n",
    "        else:\n",
    "            print(f\"[!] Image not found for mask: {mask_path}\")\n",
    "\n",
    "    return zip(*image_mask_pairs)\n",
    "\n",
    "\n",
    "# ----------------- Training -----------------\n",
    "def train_model():\n",
    "    image_paths, mask_paths = get_valid_image_mask_pairs()\n",
    "    image_paths, mask_paths = list(image_paths), list(mask_paths)\n",
    "\n",
    "    if len(image_paths) == 0:\n",
    "        raise RuntimeError(\"No valid image-mask pairs found.\")\n",
    "\n",
    "    print(f\"[INFO] Total valid image-mask pairs: {len(image_paths)}\")\n",
    "\n",
    "    # Split dataset into train and validation\n",
    "    split = int(0.8 * len(image_paths))\n",
    "    train_imgs, val_imgs = image_paths[:split], image_paths[split:]\n",
    "    train_masks, val_masks = mask_paths[:split], mask_paths[split:]\n",
    "\n",
    "    # Apply transformations (data augmentation)\n",
    "    train_tf = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    val_tf = A.Compose([\n",
    "        A.Resize(128, 128),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    # Create datasets and dataloaders\n",
    "    train_ds = LeafSegmentationDataset(train_imgs, train_masks, train_tf)\n",
    "    val_ds = LeafSegmentationDataset(val_imgs, val_masks, val_tf)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_ds, batch_size=2, shuffle=False, num_workers=0)\n",
    "\n",
    "    # Initialize U-Net model\n",
    "    model = smp.Unet(\n",
    "        encoder_name=\"resnet34\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    # Loss function and optimizer\n",
    "    loss_fn = smp.losses.DiceLoss(mode=\"binary\")\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for imgs, masks in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            preds = model(imgs)\n",
    "            loss = loss_fn(preds, masks)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1} Loss: {epoch_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    # Save the model\n",
    "    torch.save(model.state_dict(), \"pretrained_unet_leaf.pth\")\n",
    "\n",
    "\n",
    "# ----------------- Prediction -----------------\n",
    "def predict(image_path, model_path=\"pretrained_unet_leaf.pth\", save_path=None):\n",
    "    tf = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    input_tensor = tf(image=image)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_tensor)[0, 0].cpu().numpy()\n",
    "        mask = (pred > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "    if save_path:\n",
    "        Image.fromarray(mask).save(save_path)\n",
    "        print(f\"Saved predicted mask to {save_path}\")\n",
    "\n",
    "    # Visualize\n",
    "    overlay = image.copy()\n",
    "    overlay[mask == 255] = [255, 0, 0]  # Red overlay\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(\"Original\")\n",
    "    plt.imshow(image)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(\"Predicted Mask\")\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# ----------------- Run Training -----------------\n",
    "train_model()\n",
    "\n",
    "# ----------------- Run Prediction Example -----------------\n",
    "# predict(\"Plant_Disease_Dataset_Unified/train/images/Apple___Apple_scab/<your_file>.JPG\", save_path=\"output_mask.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd50b369-3c3e-4a4b-b1dc-905f8d115308",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example prediction (adjust path if needed)\n",
    "image_path = r\"./Plant_Disease_Dataset_Unified/train/images/Apple___Apple_scab/76eca386-c7c1-4807-ae27-13c793d06e02___FREC_Scab 3272_90deg.JPG\"\n",
    "\n",
    "predict(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492c8d0-7bfc-4d61-ac30-f8d31c315fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "image_path = \"./Plant_Disease_Dataset_Unified/train/images/Apple___Apple_scab/74dfd7d6-e78e-47d9-8530-35acab5cd01f___JR_FrgE.S 3027_90deg.jpg\"\n",
    "if os.path.exists(image_path):\n",
    "    print(f\"File exists: {image_path}\")\n",
    "else:\n",
    "    print(f\"File does not exist at: {image_path}\")\n",
    "    files = os.listdir(\"./Plant_Disease_Dataset_Unified/train/images/Apple___Apple_scab\")\n",
    "    print(\"Files in folder:\")\n",
    "    for f in files:\n",
    "        print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1bcc7ed-abfa-48cf-8885-b95a1cd844e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "# ----------------- Prediction -----------------\n",
    "def predict_and_save_mask(image_path, model_path=\"pretrained_unet_leaf.pth\", save_path=None):\n",
    "    tf = A.Compose([\n",
    "        A.Resize(256, 256),\n",
    "        A.Normalize(),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    image = np.array(Image.open(image_path).convert(\"RGB\"))\n",
    "    input_tensor = tf(image=image)[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "    model = smp.Unet(\"resnet34\", encoder_weights=\"imagenet\", in_channels=3, classes=1).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(input_tensor)[0, 0].cpu().numpy()\n",
    "        mask = (pred > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "    if save_path:\n",
    "        Image.fromarray(mask).save(save_path)\n",
    "        print(f\"Saved predicted mask to {save_path}\")\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "# ----------------- Generate Masks for Images Without Masks -----------------\n",
    "def generate_masks_for_images_without_masks(image_dir, mask_dir, output_mask_dir, model_path=\"pretrained_unet_leaf.pth\"):\n",
    "    # Get list of images and their corresponding masks\n",
    "    image_paths = glob(os.path.join(image_dir, \"**\", \"*.jpg\"), recursive=True)\n",
    "    mask_paths = glob(os.path.join(mask_dir, \"**\", \"*.png\"), recursive=True)\n",
    "    mask_names = [os.path.splitext(os.path.basename(mask_path))[0] for mask_path in mask_paths]\n",
    "\n",
    "    # Loop through all images\n",
    "    for image_path in image_paths:\n",
    "        mask_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "\n",
    "        # Check if this image has a corresponding mask\n",
    "        if mask_name not in mask_names:\n",
    "            # If no mask exists, generate the mask\n",
    "            print(f\"[INFO] Generating mask for {image_path}\")\n",
    "\n",
    "            # Get the class name (assumes class name is the immediate subdirectory of the image)\n",
    "            class_name = os.path.basename(os.path.dirname(image_path))\n",
    "\n",
    "            # Create corresponding directory for predicted masks\n",
    "            class_output_dir = os.path.join(output_mask_dir, class_name)\n",
    "            os.makedirs(class_output_dir, exist_ok=True)\n",
    "\n",
    "            # Save the predicted mask in the class-specific directory\n",
    "            generated_mask_path = os.path.join(class_output_dir, f\"{mask_name}_predicted.png\")\n",
    "            predict_and_save_mask(image_path, model_path, save_path=generated_mask_path)\n",
    "        else:\n",
    "            print(f\"[INFO] Mask already exists for {image_path}\")\n",
    "\n",
    "\n",
    "# ----------------- Run Prediction for All Images Without Masks -----------------\n",
    "image_dir = \"./Plant_Disease_Dataset_Unified/train/images\"\n",
    "mask_dir = \"./dataset/masks\"\n",
    "output_mask_dir = \"./predicted_masks\"  # Directory for predicted masks\n",
    "\n",
    "generate_masks_for_images_without_masks(image_dir, mask_dir, output_mask_dir, model_path=\"pretrained_unet_leaf.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36045ad6-57ca-4256-8d72-6d7b48954510",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "# ----------------- Load and Display Actual Image, Predicted Mask, and Overlay -----------------\n",
    "def show_images_and_masks(image_dir, output_mask_dir, num_images_per_class=5):\n",
    "    # Get list of all image classes\n",
    "    class_names = [os.path.basename(class_dir) for class_dir in glob(os.path.join(image_dir, \"*\"))]\n",
    "\n",
    "    for class_name in class_names:\n",
    "        print(f\"[INFO] Displaying images and predicted masks for class: {class_name}\")\n",
    "        \n",
    "        # Get image paths and predicted mask paths for the current class\n",
    "        image_paths = sorted(glob(os.path.join(image_dir, class_name, \"*.jpg\")))\n",
    "        predicted_mask_paths = sorted(glob(os.path.join(output_mask_dir, class_name, \"*.png\")))\n",
    "        \n",
    "        # Limit number of images\n",
    "        image_paths = image_paths[:num_images_per_class]\n",
    "        predicted_mask_paths = predicted_mask_paths[:num_images_per_class]\n",
    "\n",
    "        if len(image_paths) != len(predicted_mask_paths):\n",
    "            print(f\"[!] Mismatch in image and mask count for class: {class_name}\")\n",
    "            continue\n",
    "\n",
    "        num = len(image_paths)\n",
    "        plt.figure(figsize=(18, 6 * num))  # Bigger figure\n",
    "\n",
    "        for idx, (img_path, mask_path) in enumerate(zip(image_paths, predicted_mask_paths)):\n",
    "            image = np.array(Image.open(img_path).convert(\"RGB\"))\n",
    "            mask = np.array(Image.open(mask_path))\n",
    "\n",
    "            # Create overlay\n",
    "            overlay = image.copy()\n",
    "            overlay[mask > 0] = [255, 0, 0]  # Red mask\n",
    "\n",
    "            # Plot original image\n",
    "            plt.subplot(num, 3, 3 * idx + 1)\n",
    "            plt.imshow(image)\n",
    "            plt.title(\"Original Image\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Plot predicted mask\n",
    "            plt.subplot(num, 3, 3 * idx + 2)\n",
    "            plt.imshow(mask, cmap=\"gray\")\n",
    "            plt.title(\"Predicted Mask\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "            # Plot overlay\n",
    "            plt.subplot(num, 3, 3 * idx + 3)\n",
    "            plt.imshow(overlay)\n",
    "            plt.title(\"Overlay\")\n",
    "            plt.axis(\"off\")\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ----------------- Run Display Function -----------------\n",
    "image_dir = \"./Plant_Disease_Dataset_Unified/train/images\"\n",
    "output_mask_dir = \"./predicted_masks\"\n",
    "\n",
    "show_images_and_masks(image_dir, output_mask_dir, num_images_per_class=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "622ddee7-e620-4aa2-b975-fc0c374cd033",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
